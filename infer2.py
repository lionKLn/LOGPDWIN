import joblib
import numpy as np
import pandas as pd
import torch
from transformers import AutoTokenizer, AutoModel
import logging
from typing import Optional, Union
import argparse
from pathlib import Path
import time

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class InferenceProcessor:
    def __init__(
            self,
            encoder_path: str = 'encoder.pkl',
            encoder_columns_path: str = 'encoder_columns.npy',
            codebert_path: str = "./codebert",
            device: Optional[str] = None
    ):
        """
        åˆå§‹åŒ–ç‰¹å¾å¤„ç†å™¨
        :param encoder_path: OneHotEncoderä¿å­˜è·¯å¾„
        :param encoder_columns_path: ç¼–ç åˆ—åä¿å­˜è·¯å¾„
        :param codebert_path: CodeBERTæ¨¡å‹è·¯å¾„
        :param device: æŒ‡å®šè®¾å¤‡ (cuda/cpu)
        """
        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')
        self.onehot_fields = ['oracle_name', 'sut.component', 'sut.component_set', 'sut.module']
        self.codebert_dim = 768

        # åŠ è½½é¢„è®­ç»ƒç»„ä»¶
        self._load_artifacts(encoder_path, encoder_columns_path, codebert_path)

    def _load_artifacts(self, encoder_path: str, encoder_columns_path: str, codebert_path: str):
        """åŠ è½½æ‰€æœ‰é¢„è®­ç»ƒç»„ä»¶"""
        try:
            logger.info("ğŸ”„ æ­£åœ¨åŠ è½½é¢„è®­ç»ƒç»„ä»¶...")

            # éªŒè¯æ–‡ä»¶å­˜åœ¨
            if not Path(encoder_path).exists():
                raise FileNotFoundError(f"Encoder file not found: {encoder_path}")
            if not Path(encoder_columns_path).exists():
                raise FileNotFoundError(f"Encoder columns file not found: {encoder_columns_path}")

            # åŠ è½½OneHotç¼–ç å™¨
            self.onehot_encoder = joblib.load(encoder_path)
            self.expected_onehot_columns = np.load(encoder_columns_path, allow_pickle=True)
            logger.info(f"âœ… æˆåŠŸåŠ è½½OneHotEncoderï¼Œç‰¹å¾æ•°: {len(self.expected_onehot_columns)}")

            # åŠ è½½CodeBERTæ¨¡å‹
            if not Path(codebert_path).exists():
                raise FileNotFoundError(f"CodeBERT path not found: {codebert_path}")

            start_time = time.time()
            self.tokenizer = AutoTokenizer.from_pretrained(codebert_path)
            self.codebert_model = AutoModel.from_pretrained(codebert_path).to(self.device)
            self.codebert_model.eval()
            load_time = time.time() - start_time
            logger.info(f"âœ… æˆåŠŸåŠ è½½CodeBERTæ¨¡å‹ (è®¾å¤‡: {self.device}, è€—æ—¶: {load_time:.2f}s)")

        except Exception as e:
            logger.error(f"âŒ åŠ è½½ç»„ä»¶å¤±è´¥: {str(e)}")
            raise

    def _validate_input(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        éªŒè¯å¹¶é¢„å¤„ç†è¾“å…¥æ•°æ®
        :return: å¤„ç†åçš„DataFrame
        """
        # æ£€æŸ¥å¿…éœ€å­—æ®µ
        required_cols = self.onehot_fields + ['api_ut', 'tags']
        missing_cols = set(required_cols) - set(df.columns)
        if missing_cols:
            raise ValueError(f"âŒ ç¼ºå°‘å¿…è¦åˆ—: {missing_cols}")

        # å¤åˆ¶æ•°æ®é¿å…ä¿®æ”¹åŸå§‹æ•°æ®
        processed_df = df.copy()

        # å¡«å……ç¼ºå¤±å€¼
        for col in self.onehot_fields:
            if processed_df[col].isna().any():
                na_count = processed_df[col].isna().sum()
                logger.warning(f"âš ï¸ åœ¨åˆ— {col} ä¸­å‘ç° {na_count} ä¸ªNAå€¼ï¼Œå°†å¡«å……ä¸º'missing'")
                processed_df[col] = processed_df[col].fillna('missing')

        return processed_df

    def _encode_text_batch(self, texts: pd.Series, batch_size: int = 32) -> np.ndarray:
        """
        æ‰¹é‡ç¼–ç æ–‡æœ¬å­—æ®µ
        :param texts: æ–‡æœ¬åºåˆ—
        :param batch_size: æ‰¹å¤„ç†å¤§å°
        :return: numpyæ•°ç»„ (n_samples, 768)
        """
        embeddings = []
        texts = texts.fillna('').astype(str).tolist()
        total_batches = (len(texts) // batch_size + (1 if len(texts) % batch_size else 0)

                         for i in range(0, len(texts), batch_size):
        batch = texts[i:i + batch_size]
        batch_num = (i // batch_size) + 1
        try:
            start_time = time.time()
            with torch.no_grad():
                inputs = self.tokenizer(
                    batch,
                    padding=True,
                    truncation=True,
                    max_length=128,
                    return_tensors="pt"
                ).to(self.device)

                outputs = self.codebert_model(**inputs)
                batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()
                embeddings.append(batch_embeddings)

            batch_time = time.time() - start_time
            logger.debug(f"ğŸ”§ å¤„ç†æ‰¹æ¬¡ {batch_num}/{total_batches} (å¤§å°: {len(batch)}), è€—æ—¶: {batch_time:.2f}s")

        except Exception as e:
            logger.error(f"âŒ å¤„ç†æ‰¹æ¬¡ {batch_num} å¤±è´¥: {str(e)}")
            # å¤±è´¥æ—¶è¿”å›é›¶å‘é‡
            embeddings.append(np.zeros((len(batch), self.codebert_dim)))

        return np.vstack(embeddings)

    def process_features(self, df: pd.DataFrame) -> np.ndarray:
        """
        å®Œæ•´ç‰¹å¾å¤„ç†æµæ°´çº¿
        :param df: è¾“å…¥DataFrame
        :return: å¤„ç†åçš„ç‰¹å¾çŸ©é˜µ
        """
        logger.info("ğŸ› ï¸ å¼€å§‹ç‰¹å¾å¤„ç†æµç¨‹...")

        # 1. æ•°æ®éªŒè¯ä¸é¢„å¤„ç†
        processed_df = self._validate_input(df)
        logger.info(f"ğŸ“Š è¾“å…¥æ•°æ®å½¢çŠ¶: {processed_df.shape}")

        # 2. OneHotç¼–ç 
        try:
            logger.info("ğŸ”  æ­£åœ¨è¿›è¡ŒOneHotç¼–ç ...")
            onehot_data = self.onehot_encoder.transform(processed_df[self.onehot_fields])
            onehot_df = pd.DataFrame(
                onehot_data,
                columns=self.onehot_encoder.get_feature_names_out()
            )
            # å¯¹é½ç‰¹å¾åˆ—
            onehot_df = onehot_df.reindex(columns=self.expected_onehot_columns, fill_value=0)
            logger.info(f"âœ… OneHotç¼–ç å®Œæˆï¼Œå½¢çŠ¶: {onehot_df.shape}")
        except Exception as e:
            logger.error(f"âŒ OneHotç¼–ç å¤±è´¥: {str(e)}")
            raise

        # 3. æ–‡æœ¬ç¼–ç 
        try:
            logger.info("ğŸ“ æ­£åœ¨ç¼–ç æ–‡æœ¬å­—æ®µ(api_ut)...")
            api_ut_embeds = self._encode_text_batch(processed_df['api_ut'])
            logger.info("ğŸ·ï¸ æ­£åœ¨ç¼–ç æ–‡æœ¬å­—æ®µ(tags)...")
            tag_embeds = self._encode_text_batch(processed_df['tags'])
            logger.info(f"âœ… æ–‡æœ¬ç¼–ç å®Œæˆï¼Œå½¢çŠ¶: api_ut={api_ut_embeds.shape}, tags={tag_embeds.shape}")
        except Exception as e:
            logger.error(f"âŒ æ–‡æœ¬ç¼–ç å¤±è´¥: {str(e)}")
            raise

        # 4. åˆå¹¶ç‰¹å¾
        X = np.hstack([onehot_df.values, api_ut_embeds, tag_embeds])
        logger.info(f"ğŸ‰ ç‰¹å¾å¤„ç†å®Œæˆ! æœ€ç»ˆç‰¹å¾çŸ©é˜µå½¢çŠ¶: {X.shape}")

        return X


def load_data(file_path: str) -> pd.DataFrame:
    """åŠ è½½è¾“å…¥æ•°æ®æ–‡ä»¶"""
    try:
        logger.info(f"ğŸ“‚ æ­£åœ¨åŠ è½½æ•°æ®æ–‡ä»¶: {file_path}")

        # æ”¯æŒå¤šç§æ–‡ä»¶æ ¼å¼
        if file_path.endswith('.csv'):
            df = pd.read_csv(file_path)
        elif file_path.endswith('.parquet'):
            df = pd.read_parquet(file_path)
        elif file_path.endswith('.json'):
            df = pd.read_json(file_path)
        else:
            raise ValueError("ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ï¼Œè¯·ä½¿ç”¨CSV/Parquet/JSON")

        logger.info(f"âœ… æˆåŠŸåŠ è½½æ•°æ®ï¼Œè¡Œæ•°: {len(df)}")
        return df
    except Exception as e:
        logger.error(f"âŒ åŠ è½½æ•°æ®æ–‡ä»¶å¤±è´¥: {str(e)}")
        raise


def main():
    """ä¸»å¤„ç†æµç¨‹"""
    parser = argparse.ArgumentParser(description='æµ‹è¯•æ•°æ®ç‰¹å¾å¤„ç†å™¨')
    parser.add_argument('--input', type=str, required=True, help='è¾“å…¥æ–‡ä»¶è·¯å¾„(CSV/Parquet/JSON)')
    parser.add_argument('--output', type=str, default='output_features.npy', help='ç‰¹å¾è¾“å‡ºè·¯å¾„')
    parser.add_argument('--encoder', type=str, default='encoder.pkl', help='OneHotEncoderè·¯å¾„')
    parser.add_argument('--encoder-cols', type=str, default='encoder_columns.npy', help='ç¼–ç åˆ—åè·¯å¾„')
    parser.add_argument('--codebert', type=str, default='./codebert', help='CodeBERTæ¨¡å‹è·¯å¾„')
    args = parser.parse_args()

    try:
        # 1. åŠ è½½æ•°æ®
        df = load_data(args.input)

        # 2. åˆå§‹åŒ–å¤„ç†å™¨
        processor = InferenceProcessor(
            encoder_path=args.encoder,
            encoder_columns_path=args.encoder_cols,
            codebert_path=args.codebert
        )

        # 3. å¤„ç†ç‰¹å¾
        start_time = time.time()
        features = processor.process_features(df)
        process_time = time.time() - start_time

        # 4. ä¿å­˜ç»“æœ
        np.save(args.output, features)
        logger.info(f"ğŸ’¾ ç‰¹å¾å·²ä¿å­˜åˆ° {args.output}ï¼Œæ€»è€—æ—¶: {process_time:.2f}ç§’")

    except Exception as e:
        logger.error(f"ğŸ”¥ å¤„ç†å¤±è´¥: {str(e)}")
        exit(1)


if __name__ == "__main__":
    main()