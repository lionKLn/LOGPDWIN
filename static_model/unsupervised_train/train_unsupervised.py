import os
import torch
from torch_geometric.loader import DataLoader
import lightning.pytorch as pl
from lightning.pytorch.callbacks import ModelCheckpoint

# å¯¼å…¥NPUç›¸å…³ä¾èµ–ï¼ˆè‹¥æœªå®‰è£…éœ€å…ˆæ‰§è¡Œï¼špip install torch-npuï¼‰
try:
    import torch_npu
except ImportError:
    raise ImportError("è¯·å…ˆå®‰è£…torch-npuä»¥æ”¯æŒNPUè®¾å¤‡ï¼Œå®‰è£…å‘½ä»¤ï¼špip install torch-npu")

from unsupervised_train.model import GAE_GIN_lightning
from unsupervised_train.dataloader import GraphDataset


def main():
    # 1. åŸºç¡€é…ç½®
    data_dir = "../data/graph_dataset"  # å›¾æ•°æ®ç›®å½•ï¼ˆ.ptæ–‡ä»¶ï¼‰
    batch_size = 8  # æ‰¹å¤§å°
    num_workers = 4  # æ•°æ®åŠ è½½è¿›ç¨‹æ•°
    target_npu = "npu:6"  # ç›®æ ‡NPUå¡å·ï¼ˆå¯æ ¹æ®éœ€æ±‚ä¿®æ”¹ï¼‰
    max_epochs = 20  # è®­ç»ƒè½®æ¬¡
    checkpoint_dir = "./checkpoints"  # æƒé‡ä¿å­˜ç›®å½•

    # 2. è®¾å¤‡æ£€æµ‹ä¸é…ç½®
    # æ£€æŸ¥NPUæ˜¯å¦å¯ç”¨ï¼Œä¼˜å…ˆä½¿ç”¨æŒ‡å®šçš„NPUï¼Œå¦åˆ™é™çº§åˆ°GPU/CPU
    npu_available = torch.npu.is_available() and target_npu in [f"npu:{i}" for i in range(torch.npu.device_count())]
    if npu_available:
        device = torch.device(target_npu)
        accelerator = "npu"
        devices = [int(target_npu.split(":")[-1])]  # Lightningéœ€è¦ä¼ å…¥å¡å·åˆ—è¡¨
        print(f"âœ… æ£€æµ‹åˆ°å¯ç”¨NPUè®¾å¤‡ï¼š{target_npu}ï¼Œå°†ä½¿ç”¨è¯¥è®¾å¤‡è®­ç»ƒ")
    else:
        # è‹¥NPUä¸å¯ç”¨ï¼Œé™çº§åˆ°GPU/CPU
        if torch.cuda.is_available():
            device = torch.device("cuda:0")
            accelerator = "gpu"
            devices = [0]
            print(f"âš ï¸ æŒ‡å®šçš„NPUè®¾å¤‡{target_npu}ä¸å¯ç”¨ï¼Œé™çº§ä½¿ç”¨GPU:0")
        else:
            device = torch.device("cpu")
            accelerator = "cpu"
            devices = "auto"
            print(f"âš ï¸ NPUå’ŒGPUå‡ä¸å¯ç”¨ï¼Œä½¿ç”¨CPUè®­ç»ƒï¼ˆè®­ç»ƒé€Ÿåº¦å¯èƒ½è¾ƒæ…¢ï¼‰")

    # è®¾ç½®é»˜è®¤è®¾å¤‡ï¼ˆç¡®ä¿æ•°æ®å’Œæ¨¡å‹éƒ½åŠ è½½åˆ°æŒ‡å®šè®¾å¤‡ï¼‰
    torch.npu.set_device(device) if npu_available else None

    # 3. æ•°æ®åŠ è½½
    # åŠ è½½å›¾æ•°æ®é›†ï¼Œè‡ªåŠ¨è¯»å–ç›®å½•ä¸‹æ‰€æœ‰.ptæ–‡ä»¶
    train_dataset = GraphDataset(data_dir)
    # æ„å»ºDataLoaderï¼Œæ‰¹é‡åŠ è½½æ•°æ®ï¼ˆshuffle=Trueæ‰“ä¹±è®­ç»ƒæ•°æ®ï¼‰
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=True  # å¼€å¯å†…å­˜é”å®šï¼ŒåŠ é€Ÿæ•°æ®ä¼ è¾“åˆ°NPU/GPU
    )
    print(f"âœ… æˆåŠŸåŠ è½½æ•°æ®é›†ï¼šå…±{len(train_dataset)}ä¸ªå›¾æ ·æœ¬ï¼Œæ‰¹å¤§å°={batch_size}")

    # 4. æ¨¡å‹åˆå§‹åŒ–ï¼ˆå…³é”®ï¼šin_channelséœ€ä¸data.xç»´åº¦ä¸€è‡´ï¼‰
    # è‹¥data.xæ˜¯CodeBERTç¼–ç ï¼ˆ768ç»´ï¼‰ï¼Œéœ€å°†in_channelsæ”¹ä¸º768ï¼›è‹¥ä¸ºå…¶ä»–ç»´åº¦åˆ™å¯¹åº”ä¿®æ”¹
    model = GAE_GIN_lightning(
        in_channels=768,  # é‡ç‚¹ï¼šå¿…é¡»ä¸data.xçš„ç»´åº¦ä¸€è‡´ï¼ˆåŸä»£ç 128éœ€æ ¹æ®å®é™…æ•°æ®ä¿®æ”¹ï¼‰
        out_channels=768,  # ä¿ç•™æ¥å£å…¼å®¹æ€§ï¼Œæ— å®é™…ä½œç”¨
        batch_size=batch_size,
        encoder_kwargs={
            "num_gc_layers": 2,  # GINå±‚æ•°ï¼ˆå¯è°ƒæ•´ï¼‰
            "hidden_dim": 128  # GINæ¯å±‚è¾“å‡ºç»´åº¦ï¼ˆå¯è°ƒæ•´ï¼‰
        }
    )
    # å°†æ¨¡å‹ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
    model.to(device)
    print(f"âœ… æ¨¡å‹åˆå§‹åŒ–å®Œæˆï¼Œå·²åŠ è½½åˆ°{device}è®¾å¤‡")

    checkpoint_callback = ModelCheckpoint(
        monitor="train_loss",  # ç›‘æ§æŒ‡æ ‡
        save_top_k=1,  # ä¿å­˜æœ€ä¼˜çš„ä¸€ä¸ªæ¨¡å‹
        mode="min",  # è¶Šå°è¶Šå¥½
        dirpath=os.path.join(checkpoint_dir, "best"),  # ä¿å­˜ç›®å½•
        filename="gin-unsupervised-best",  # æ–‡ä»¶å
    )

    # 5. è®­ç»ƒå™¨é…ç½®ï¼ˆé€‚é…NPUï¼‰
    trainer = pl.Trainer(
        max_epochs=max_epochs,
        accelerator=accelerator,  # è®¾å¤‡ç±»å‹ï¼šnpu/gpu/cpu
        devices=devices,  # å…·ä½“è®¾å¤‡å·ï¼šNPU/GPUå¡å·åˆ—è¡¨ï¼ŒCPUè®¾ä¸ºauto
        log_every_n_steps=10,  # æ¯10æ­¥è®°å½•ä¸€æ¬¡æ—¥å¿—
        default_root_dir=checkpoint_dir,  # æƒé‡å’Œæ—¥å¿—ä¿å­˜æ ¹ç›®å½•
        enable_checkpointing=True,  # å¼€å¯æƒé‡ä¿å­˜
        callbacks=[checkpoint_callback],  # âœ… é€šè¿‡callbacksåˆ—è¡¨ä¼ å…¥
    )

    # 6. å¯åŠ¨è®­ç»ƒ
    print("ğŸš€ å¼€å§‹æ— ç›‘ç£è®­ç»ƒ...")
    print(f"ğŸ“Š è®­ç»ƒé…ç½®ï¼šè®¾å¤‡={device}ï¼Œè½®æ¬¡={max_epochs}ï¼Œæ‰¹å¤§å°={batch_size}")
    trainer.fit(model, train_loader)
    print("ğŸ¯ è®­ç»ƒå®Œæˆï¼æœ€ä¼˜æƒé‡å·²ä¿å­˜åˆ°ï¼š", os.path.join(checkpoint_dir, "best"))


if __name__ == "__main__":
    main()