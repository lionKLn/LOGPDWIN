import torch
import pandas as pd
import numpy as np
import torch.nn.functional as F
from model import LogClassifier
import os




import os
import json
import pandas as pd
import torch
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModel
from torch_geometric.data import Batch
from unsupervised_train.preprocess import generate_graph_in_memory
from model import GAE_GIN

# ----------------------------
# é…ç½®å‚æ•°
# ----------------------------
INPUT_EXCEL = "path/to/your/input.xlsx"  # å¾…åˆ†ææ•°æ®
UNSUPERVISED_MODEL_PATH = "logs/pdg/2025-05-20_14-30-00/best_pdg.pt"
DEVICE = torch.device("npu:4" if torch.npu.is_available() else "cpu")
EMBEDDING_DIM = 256

# ----------------------------
# 1. åŠ è½½Excelä¸è§£æJSON
# ----------------------------
df = pd.read_excel(INPUT_EXCEL)

results = []
for i, row in df.iterrows():
    try:
        data = json.loads(row["data"])
        raw_code = str(data.get("code_str", "")).strip()
        # code_str é¢„å¤„ç†
        if raw_code.startswith('('):
            processed_code = raw_code
        elif raw_code.startswith('{'):
            processed_code = f"(){raw_code}"
        else:
            processed_code = f"(){{{raw_code}}}"

        results.append({
            "component": data.get("component", ""),
            "code_str": processed_code,
            "raw_code": raw_code,
            "Desc": data.get("desc", ""),
            "Func": data.get("func", ""),
            "case_id": data.get("case_id", ""),
            "test_suite": data.get("test_suite", ""),
            "case_spce": data.get("case_spce", ""),
            "case_purpose": data.get("case_purpose", "")
        })
    except Exception as e:
        print(f"ç¬¬ {i} è¡Œ JSON è§£æå¤±è´¥: {e}")
        results.append({
            "component": "", "code_str": "", "raw_code": "",
            "Desc": "", "Func": "", "case_id": "", "test_suite": "",
            "case_spce": "", "case_purpose": ""
        })

merged_df = pd.concat([df, pd.DataFrame(results)], axis=1)

# ----------------------------
# 2. ç”Ÿæˆä»£ç å›¾å¹¶ç¼–ç ï¼ˆæ— ç›‘ç£æ¨¡å‹ï¼‰
# ----------------------------
print("å†…å­˜ä¸­ç”Ÿæˆ code_str çš„ä»£ç å›¾...")
graph_list = []
for idx, row in tqdm(merged_df.iterrows(), total=len(merged_df), desc="ç”Ÿæˆä»£ç å›¾"):
    processed_code = row["code_str"]
    if not processed_code:
        graph_list.append(None)
        continue

    torch_graph = generate_graph_in_memory(
        code_str=processed_code,
        func_name=f"func_{idx}"
    )
    graph_list.append(torch_graph)

print("åŠ è½½æ— ç›‘ç£å›¾ç¼–ç æ¨¡å‹...")
graph_model = GAE_GIN(
    in_channels=768,
    out_channels=768,
    device=DEVICE
).to(DEVICE)
graph_model.load_state_dict(torch.load(UNSUPERVISED_MODEL_PATH, map_location=DEVICE))
graph_model.eval()

print("ç¼–ç ä»£ç å›¾...")
code_embeddings = []
batch_size = 32

with torch.no_grad():
    for batch_start in tqdm(range(0, len(graph_list), batch_size), desc="ç¼–ç ä»£ç å›¾"):
        batch_graphs = graph_list[batch_start:batch_start + batch_size]
        valid_graphs, valid_indices = [], []
        for idx_in_batch, g in enumerate(batch_graphs):
            if g is not None:
                valid_graphs.append(g)
                valid_indices.append(idx_in_batch)

        batch_emb = [torch.zeros(EMBEDDING_DIM, device=DEVICE) for _ in batch_graphs]
        if valid_graphs:
            batch = Batch.from_data_list(valid_graphs).to(DEVICE)
            valid_embs = graph_model.forward(batch, mode="predict")
            for idx_in_batch, emb in zip(valid_indices, valid_embs):
                batch_emb[idx_in_batch] = emb

        batch_emb_cpu = [emb.cpu().tolist() for emb in batch_emb]
        code_embeddings.extend(batch_emb_cpu)

merged_df["code_embedding"] = code_embeddings

# ----------------------------
# 3. ç¼–ç æ–‡æœ¬å­—æ®µ
# ----------------------------
MODEL_PATH = "./models/paraphrase-multilingual-MiniLM-L12-v2"
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
text_model = AutoModel.from_pretrained(MODEL_PATH).to(DEVICE)
text_model.eval()

def mean_pooling(model_output, attention_mask):
    token_embeddings = model_output.last_hidden_state
    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, dim=1)
    sum_mask = torch.clamp(input_mask_expanded.sum(dim=1), min=1e-9)
    return sum_embeddings / sum_mask

def encode_texts(texts, tokenizer, model, device, batch_size=32, max_length=128, show_progress=True):
    all_embeddings = []
    with torch.no_grad():
        iterator = tqdm(range(0, len(texts), batch_size), desc="ç¼–ç æ–‡æœ¬") if show_progress else range(0, len(texts), batch_size)
        for start in iterator:
            batch_texts = texts[start:start + batch_size]
            encoded = tokenizer(batch_texts, padding=True, truncation=True, max_length=max_length, return_tensors="pt")
            input_ids, attention_mask = encoded["input_ids"].to(device), encoded["attention_mask"].to(device)
            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            sentence_embeddings = mean_pooling(outputs, attention_mask).cpu().tolist()
            all_embeddings.extend(sentence_embeddings)
    return all_embeddings

for col in ["Desc", "Func", "case_spce", "case_purpose"]:
    texts = merged_df[col].fillna("").astype(str).tolist()
    merged_df[col + "_embedding"] = encode_texts(texts, tokenizer, text_model, DEVICE)

# ----------------------------
# 4. One-hot ç¼–ç 
# ----------------------------
component_onehot = pd.get_dummies(merged_df["component"], prefix="component")
case_id_onehot = pd.get_dummies(merged_df["case_id"], prefix="case_id")
test_suite_onehot = pd.get_dummies(merged_df["test_suite"], prefix="test_suite")
rule_onehot = pd.get_dummies(merged_df.get("rule", pd.Series([])), prefix="rule")

merged_df = pd.concat([merged_df, component_onehot, case_id_onehot, test_suite_onehot, rule_onehot], axis=1)

# ----------------------------
# 5. ç‰¹å¾èåˆï¼ˆæ— æ ‡ç­¾ï¼‰
# ----------------------------
def merge_features(row):
    code_emb = row["code_embedding"]
    text_embs = []
    for col in ["Desc_embedding", "Func_embedding", "case_spce_embedding", "case_purpose_embedding"]:
        text_embs.extend(row[col])
    onehot_cols = [c for c in row.index if c.startswith(("component_", "case_id_", "test_suite_", "rule_"))]
    onehot_embs = row[onehot_cols].tolist()
    return code_emb + text_embs + onehot_embs

merged_df["merged_features"] = merged_df.apply(merge_features, axis=1)

# ----------------------------
# 6. ä¿å­˜ç»“æœï¼ˆæ— æ ‡ç­¾æ•°æ®ï¼‰
# ----------------------------
processed_data_path = "data_to_infer.pkl"
merged_df.to_pickle(processed_data_path)
print(f"âœ… å¾…åˆ†ææ•°æ®å·²å¤„ç†å®Œæˆï¼Œä¿å­˜è‡³ {processed_data_path}")


# ========================
# ğŸ”§ æ¨ç†é…ç½®
# ========================
MODEL_PATH = "best_log_classifier.pt"   # æ¨¡å‹è·¯å¾„
DATA_PATH = "data_to_infer.pkl"              # å·²ç¼–ç çš„æ–°æ•°æ®è·¯å¾„
DEVICE = torch.device("npu:5" if torch.npu.is_available() else "cpu")
HIDDEN_DIM = 128                        # ä¸è®­ç»ƒæ—¶ä¸€è‡´
OUTPUT_PATH = "inference_results.csv"   # è¾“å‡ºç»“æœæ–‡ä»¶è·¯å¾„


# ========================
# ğŸ”¹ æ•°æ®åŠ è½½å‡½æ•°
# ========================
def load_new_data(data_path):
    """
    åŠ è½½æ–°æ•°æ®å¹¶è½¬æ¢ä¸ºtensoræ ¼å¼
    å‡è®¾DataFrameä¸­å«æœ‰ä¸€åˆ— 'merged_features'ï¼ˆä¸è®­ç»ƒé˜¶æ®µä¸€è‡´ï¼‰
    """
    if not os.path.exists(data_path):
        raise FileNotFoundError(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")

    data = pd.read_pickle(data_path)
    X_new = torch.tensor(data["merged_features"].tolist(), dtype=torch.float32)
    print(f"âœ… å·²åŠ è½½æ–°æ•°æ®ï¼Œå…± {len(X_new)} æ¡æ ·æœ¬ã€‚")
    return data, X_new


# ========================
# ğŸ”¹ æ¨ç†å‡½æ•°
# ========================
def predict_with_prob(model_path, data_tensor, hidden_dim=128):
    """
    ä½¿ç”¨ä¿å­˜çš„æ¨¡å‹å¯¹æ–°æ•°æ®è¿›è¡Œé¢„æµ‹ï¼Œå¹¶è¾“å‡ºæ¦‚ç‡
    :param model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
    :param data_tensor: æ–°æ ·æœ¬ç‰¹å¾å¼ é‡ (shape: [N, feature_dim])
    :param hidden_dim: æ¨¡å‹éšè—å±‚ç»´åº¦ï¼ˆä¸è®­ç»ƒä¿æŒä¸€è‡´ï¼‰
    :return: (pred_labels, probs_0, probs_1)
    """
    # 1ï¸âƒ£ åŠ è½½æ¨¡å‹ç»“æ„
    input_dim = data_tensor.shape[1]
    model = LogClassifier(input_dim=input_dim, hidden_dim=hidden_dim)
    model.load_state_dict(torch.load(model_path, map_location=DEVICE))
    model.to(DEVICE)
    model.eval()

    # 2ï¸âƒ£ æ¨ç†é˜¶æ®µ
    with torch.no_grad():
        outputs = model(data_tensor.to(DEVICE))              # [N, 2]
        probs = F.softmax(outputs, dim=1).cpu().numpy()      # è½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒ
        preds = np.argmax(probs, axis=1)                     # å–æœ€å¤§æ¦‚ç‡å¯¹åº”çš„æ ‡ç­¾

    return preds, probs[:, 0], probs[:, 1]


# ========================
# ğŸ”¹ ä¸»ç¨‹åºå…¥å£
# ========================
if __name__ == "__main__":
    print("ğŸš€ å¼€å§‹æ¨¡å‹æ¨ç†...")

    # åŠ è½½æ•°æ®
    original_df, X_new = load_new_data(DATA_PATH)

    # æ¨¡å‹é¢„æµ‹
    preds, prob_0, prob_1 = predict_with_prob(MODEL_PATH, X_new, hidden_dim=HIDDEN_DIM)
    print("âœ… æ¨ç†å®Œæˆï¼")

    # ç»„åˆç»“æœ
    result_df = original_df.copy()
    result_df["pred_label"] = preds
    result_df["prob_0"] = prob_0
    result_df["prob_1"] = prob_1

    # è¾“å‡ºä¿å­˜
    result_df.to_csv(OUTPUT_PATH, index=False, encoding="utf-8-sig")

    print(f"ğŸ“„ é¢„æµ‹ç»“æœå·²ä¿å­˜è‡³ï¼š{OUTPUT_PATH}")
    print(f"æ ·ä¾‹é¢„è§ˆï¼š")
    print(result_df.head())
