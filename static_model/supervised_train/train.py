import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, random_split
import lightning.pytorch as pl
from lightning.pytorch import Trainer
from lightning.pytorch.callbacks import ModelCheckpoint, EarlyStopping
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path


# å®šä¹‰åˆ†ç±»å™¨ï¼ˆä¿æŒä½ æä¾›çš„ç®€å•å•å±‚ç»“æ„ï¼‰
class LogClassifier(nn.Module):
    def __init__(self, input_dim, hidden_dim=128):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 2)  # äºŒåˆ†ç±»ï¼š0ï¼ˆæ— ç¼ºé™·ï¼‰/1ï¼ˆæœ‰ç¼ºé™·ï¼‰
        )

    def forward(self, x):
        return self.net(x)


# å®šä¹‰æ•°æ®é›†ç±»
class DefectDataset(Dataset):
    def __init__(self, data_path):
        """
        åŠ è½½æ•´åˆåçš„ç‰¹å¾æ•°æ®
        data_path: æœ€ç»ˆå¤„ç†å¥½çš„Excelæ–‡ä»¶è·¯å¾„ï¼ˆfinal_processed_data.xlsxï¼‰
        """
        self.df = pd.read_excel(data_path)
        self.labels = self.df["false_positive"].values  # æ ‡ç­¾
        self.features = self._prepare_features()  # ç‰¹å¾çŸ©é˜µ

    def _prepare_features(self):
        """å°†å„ç±»ç‰¹å¾æ‹¼æ¥ä¸ºç»Ÿä¸€çš„ç‰¹å¾å‘é‡"""
        features_list = []

        # 1. å¤„ç†code_strç¼–ç ï¼ˆå›¾å‘é‡ï¼‰
        code_embeddings = np.array([np.array(emb) for emb in self.df["code_str_embedding"]])
        features_list.append(code_embeddings)

        # 2. å¤„ç†æ–‡æœ¬ç¼–ç ï¼ˆSentence-BERTå‘é‡ï¼‰
        text_cols = ["Desc_embedding", "Func_embedding", "case_space_embedding", "case_purpose_embedding"]
        for col in text_cols:
            embeddings = np.array([np.array(emb) for emb in self.df[col]])
            features_list.append(embeddings)

        # 3. å¤„ç†One-hotç‰¹å¾ï¼ˆç›´æ¥å–æ•°å€¼åˆ—ï¼‰
        onehot_cols = [col for col in self.df.columns if
                       col.startswith(("component_", "case_id_", "test_suite_", "rule_"))]
        onehot_features = self.df[onehot_cols].values
        features_list.append(onehot_features)

        # æ‹¼æ¥æ‰€æœ‰ç‰¹å¾ï¼ˆæŒ‰æ ·æœ¬ç»´åº¦æ‹¼æ¥ï¼‰
        return np.concatenate(features_list, axis=1).astype(np.float32)

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        return {
            "features": torch.tensor(self.features[idx], dtype=torch.float32),
            "label": torch.tensor(self.labels[idx], dtype=torch.long)
        }


# å®šä¹‰Lightningæ¨¡å—ï¼ˆå°è£…è®­ç»ƒé€»è¾‘ï¼‰
class DefectPredictor(pl.LightningModule):
    def __init__(self, input_dim, hidden_dim=128, lr=1e-4):
        super().__init__()
        self.save_hyperparameters()
        self.model = LogClassifier(input_dim, hidden_dim)
        self.loss_fn = nn.CrossEntropyLoss()  # äºŒåˆ†ç±»äº¤å‰ç†µæŸå¤±
        self.val_metrics = []  # ä¿å­˜éªŒè¯é›†æŒ‡æ ‡

    def forward(self, x):
        return self.model(x)

    def training_step(self, batch, batch_idx):
        features = batch["features"]
        labels = batch["label"]
        outputs = self(features)
        loss = self.loss_fn(outputs, labels)

        # è®¡ç®—è®­ç»ƒé›†å‡†ç¡®ç‡
        preds = torch.argmax(outputs, dim=1)
        acc = accuracy_score(labels.cpu(), preds.cpu())

        self.log("train_loss", loss, prog_bar=True, sync_dist=True)
        self.log("train_acc", acc, prog_bar=True, sync_dist=True)
        return loss

    def validation_step(self, batch, batch_idx):
        features = batch["features"]
        labels = batch["label"]
        outputs = self(features)
        loss = self.loss_fn(outputs, labels)

        preds = torch.argmax(outputs, dim=1)
        acc = accuracy_score(labels.cpu(), preds.cpu())
        precision, recall, f1, _ = precision_recall_fscore_support(
            labels.cpu(), preds.cpu(), average="binary"
        )

        self.log("val_loss", loss, prog_bar=True, sync_dist=True)
        self.log("val_acc", acc, prog_bar=True, sync_dist=True)
        self.log("val_f1", f1, prog_bar=True, sync_dist=True)

        self.val_metrics.append({
            "labels": labels.cpu(),
            "preds": preds.cpu()
        })
        return loss

    def on_validation_epoch_end(self):
        """æ¯ä¸ªéªŒè¯ epoch ç»“æŸåè®¡ç®—æ··æ·†çŸ©é˜µ"""
        all_labels = torch.cat([m["labels"] for m in self.val_metrics]).numpy()
        all_preds = torch.cat([m["preds"] for m in self.val_metrics]).numpy()

        # è®¡ç®—æ··æ·†çŸ©é˜µ
        cm = confusion_matrix(all_labels, all_preds)
        plt.figure(figsize=(8, 6))
        sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
                    xticklabels=["æ— ç¼ºé™·", "æœ‰ç¼ºé™·"],
                    yticklabels=["æ— ç¼ºé™·", "æœ‰ç¼ºé™·"])
        plt.xlabel("é¢„æµ‹æ ‡ç­¾")
        plt.ylabel("çœŸå®æ ‡ç­¾")
        plt.title(f"éªŒè¯é›†æ··æ·†çŸ©é˜µ (Epoch {self.current_epoch})")

        # ä¿å­˜æ··æ·†çŸ©é˜µå›¾ç‰‡
        Path("classifier_results/confusion_matrices").mkdir(parents=True, exist_ok=True)
        plt.savefig(f"classifier_results/confusion_matrices/epoch_{self.current_epoch}.png")
        plt.close()

        self.val_metrics.clear()  # æ¸…ç©ºç¼“å­˜

    def configure_optimizers(self):
        return optim.Adam(self.parameters(), lr=self.hparams.lr)


def main():
    # é…ç½®
    DATA_PATH = "final_processed_data.xlsx"  # æ•´åˆåçš„ç‰¹å¾æ•°æ®
    BATCH_SIZE = 32
    HIDDEN_DIM = 128
    LEARNING_RATE = 1e-4
    MAX_EPOCHS = 50
    VAL_SPLIT = 0.2  # è®­ç»ƒé›†:éªŒè¯é›† = 8:2
    device = torch.device("npu:6" if torch.npu.is_available() else
                          "cuda:0" if torch.cuda.is_available() else "cpu")

    # åˆ›å»ºè¾“å‡ºç›®å½•
    Path("classifier_results/checkpoints").mkdir(parents=True, exist_ok=True)

    # 1. åŠ è½½æ•°æ®
    dataset = DefectDataset(DATA_PATH)
    input_dim = dataset.features.shape[1]  # è‡ªåŠ¨è®¡ç®—è¾“å…¥ç‰¹å¾ç»´åº¦
    print(f"âœ… æ•°æ®åŠ è½½å®Œæˆï¼Œæ ·æœ¬æ•°: {len(dataset)}, ç‰¹å¾ç»´åº¦: {input_dim}")

    # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
    val_size = int(VAL_SPLIT * len(dataset))
    train_size = len(dataset) - val_size
    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(
        train_dataset,
        batch_size=BATCH_SIZE,
        shuffle=True,
        num_workers=4,
        pin_memory=True
    )
    val_loader = DataLoader(
        val_dataset,
        batch_size=BATCH_SIZE,
        shuffle=False,
        num_workers=4,
        pin_memory=True
    )

    # 2. åˆå§‹åŒ–æ¨¡å‹
    model = DefectPredictor(
        input_dim=input_dim,
        hidden_dim=HIDDEN_DIM,
        lr=LEARNING_RATE
    )

    # 3. é…ç½®è®­ç»ƒå™¨
    checkpoint_callback = ModelCheckpoint(
        dirpath="classifier_results/checkpoints",
        filename="best-model",
        monitor="val_f1",  # ä»¥F1åˆ†æ•°ä½œä¸ºæœ€ä¼˜æ¨¡å‹æŒ‡æ ‡ï¼ˆæ¯”å‡†ç¡®ç‡æ›´é€‚åˆä¸å¹³è¡¡æ•°æ®ï¼‰
        mode="max",
        save_top_k=1
    )

    early_stopping = EarlyStopping(
        monitor="val_f1",
        mode="max",
        patience=5,  # 5ä¸ªepochæ²¡æå‡å°±åœæ­¢
        verbose=True
    )

    trainer = Trainer(
        max_epochs=MAX_EPOCHS,
        accelerator="npu" if "npu" in str(device) else "gpu" if "cuda" in str(device) else "cpu",
        devices=[int(str(device).split(":")[-1])] if "npu" in str(device) or "cuda" in str(device) else "auto",
        callbacks=[checkpoint_callback, early_stopping],
        default_root_dir="classifier_results",
        log_every_n_steps=10
    )

    # 4. å¼€å§‹è®­ç»ƒ
    print(f"ğŸš€ å¼€å§‹è®­ç»ƒåˆ†ç±»å™¨ï¼Œè®¾å¤‡: {device}")
    trainer.fit(model, train_loader, val_loader)

    # 5. è¾“å‡ºæœ€ä½³æ¨¡å‹ä¿¡æ¯
    print(f"ğŸ¯ è®­ç»ƒå®Œæˆï¼æœ€ä½³æ¨¡å‹ä¿å­˜åœ¨: {checkpoint_callback.best_model_path}")
    print(f"æœ€ä½³éªŒè¯é›†F1åˆ†æ•°: {checkpoint_callback.best_score:.4f}")


if __name__ == "__main__":
    main()
