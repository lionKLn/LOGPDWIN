import torch
import pandas as pd
import numpy as np
import torch.nn.functional as F
from model import LogClassifier
import os
import joblib
from sklearn.preprocessing import OneHotEncoder


import os
import json
import pandas as pd
import torch
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModel
from torch_geometric.data import Batch
from unsupervised_train.preprocess import generate_graph_in_memory
from model import GAE_GIN

# ----------------------------
# é…ç½®å‚æ•°ï¼ˆå¼ºè°ƒå­—æ®µé¡ºåºå¿…é¡»ä¸è®­ç»ƒä¸€è‡´ï¼‰
# ----------------------------
INPUT_EXCEL = "path/to/your/input.xlsx"
UNSUPERVISED_MODEL_PATH = "logs/pdg/2025-05-20_14-30-00/best_pdg.pt"
DEVICE = torch.device("npu:4" if torch.npu.is_available() else "cpu")
EMBEDDING_DIM = 256
ONEHOT_ENCODER_PATH = "onehot_encoder.pkl"
ONEHOT_FEATURES_PATH = "onehot_feature_names.npy"
# å…³é”®ï¼šç¦»æ•£å­—æ®µé¡ºåºå¿…é¡»ä¸è®­ç»ƒæ—¶fitçš„é¡ºåºå®Œå…¨ä¸€è‡´ï¼ï¼ï¼
ONEHOT_FIELDS = ["component", "case_id", "test_suite", "rule"]  # é¡ºåºä¸å¯å˜


# ----------------------------
# 1. åŠ è½½Excelä¸è§£æJSONï¼Œæœ€åŸå§‹çš„excelæ–‡ä»¶
# ----------------------------
df = pd.read_excel(INPUT_EXCEL)

results = []
for i, row in df.iterrows():
    try:
        data = json.loads(row["data"])
        raw_code = str(data.get("code_str", "")).strip()
        if raw_code.startswith('('):
            processed_code = raw_code
        elif raw_code.startswith('{'):
            processed_code = f"(){raw_code}"
        else:
            processed_code = f"(){{{raw_code}}}"

        results.append({
            "component": data.get("component", ""),
            "case_id": data.get("case_id", ""),
            "test_suite": data.get("test_suite", ""),
            "rule": data.get("rule", ""),  # æŒ‰ONEHOT_FIELDSé¡ºåºæ’åˆ—å­—æ®µ
            "code_str": processed_code,
            "raw_code": raw_code,
            "Desc": data.get("desc", ""),
            "Func": data.get("func", ""),
            "case_spce": data.get("case_spce", ""),
            "case_purpose": data.get("case_purpose", "")
        })
    except Exception as e:
        print(f"ç¬¬ {i} è¡Œ JSON è§£æå¤±è´¥: {e}")
        results.append({
            "component": "", "case_id": "", "test_suite": "", "rule": "",  # æŒ‰é¡ºåºå¡«å……é»˜è®¤å€¼
            "code_str": "", "raw_code": "",
            "Desc": "", "Func": "", "case_spce": "", "case_purpose": ""
        })

merged_df = pd.concat([df, pd.DataFrame(results)], axis=1)


# ----------------------------
# 2. ç”Ÿæˆä»£ç å›¾å¹¶ç¼–ç ï¼ˆæ— ä¿®æ”¹ï¼‰
# ----------------------------
print("å†…å­˜ä¸­ç”Ÿæˆ code_str çš„ä»£ç å›¾...")
graph_list = []
for idx, row in tqdm(merged_df.iterrows(), total=len(merged_df), desc="ç”Ÿæˆä»£ç å›¾"):
    processed_code = row["code_str"]
    if not processed_code:
        graph_list.append(None)
        continue

    torch_graph = generate_graph_in_memory(
        code_str=processed_code,
        func_name=f"func_{idx}"
    )
    graph_list.append(torch_graph)

print("åŠ è½½æ— ç›‘ç£å›¾ç¼–ç æ¨¡å‹...")
graph_model = GAE_GIN(
    in_channels=768,
    out_channels=768,
    device=DEVICE
).to(DEVICE)
graph_model.load_state_dict(torch.load(UNSUPERVISED_MODEL_PATH, map_location=DEVICE))
graph_model.eval()

print("ç¼–ç ä»£ç å›¾...")
code_embeddings = []
batch_size = 32

with torch.no_grad():
    for batch_start in tqdm(range(0, len(graph_list), batch_size), desc="ç¼–ç ä»£ç å›¾"):
        batch_graphs = graph_list[batch_start:batch_start + batch_size]
        valid_graphs, valid_indices = [], []
        for idx_in_batch, g in enumerate(batch_graphs):
            if g is not None:
                valid_graphs.append(g)
                valid_indices.append(idx_in_batch)

        batch_emb = [torch.zeros(EMBEDDING_DIM, device=DEVICE) for _ in batch_graphs]
        if valid_graphs:
            batch = Batch.from_data_list(valid_graphs).to(DEVICE)
            valid_embs = graph_model.forward(batch, mode="predict")
            for idx_in_batch, emb in zip(valid_indices, valid_embs):
                batch_emb[idx_in_batch] = emb

        batch_emb_cpu = [emb.cpu().tolist() for emb in batch_emb]
        code_embeddings.extend(batch_emb_cpu)

merged_df["code_embedding"] = code_embeddings


# ----------------------------
# 3. ç¼–ç æ–‡æœ¬å­—æ®µï¼ˆæ— ä¿®æ”¹ï¼‰
# ----------------------------
MODEL_PATH = "./models/paraphrase-multilingual-MiniLM-L12-v2"
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
text_model = AutoModel.from_pretrained(MODEL_PATH).to(DEVICE)
text_model.eval()

def mean_pooling(model_output, attention_mask):
    token_embeddings = model_output.last_hidden_state
    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, dim=1)
    sum_mask = torch.clamp(input_mask_expanded.sum(dim=1), min=1e-9)
    return sum_embeddings / sum_mask

def encode_texts(texts, tokenizer, model, device, batch_size=32, max_length=128, show_progress=True):
    all_embeddings = []
    with torch.no_grad():
        iterator = tqdm(range(0, len(texts), batch_size), desc="ç¼–ç æ–‡æœ¬") if show_progress else range(0, len(texts), batch_size)
        for start in iterator:
            batch_texts = texts[start:start + batch_size]
            encoded = tokenizer(batch_texts, padding=True, truncation=True, max_length=max_length, return_tensors="pt")
            input_ids, attention_mask = encoded["input_ids"].to(device), encoded["attention_mask"].to(device)
            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            sentence_embeddings = mean_pooling(outputs, attention_mask).cpu().tolist()
            all_embeddings.extend(sentence_embeddings)
    return all_embeddings

for col in ["Desc", "Func", "case_spce", "case_purpose"]:
    texts = merged_df[col].fillna("").astype(str).tolist()
    merged_df[col + "_embedding"] = encode_texts(texts, tokenizer, text_model, DEVICE)


# ----------------------------
# 4. One-hot ç¼–ç ï¼ˆå½»åº•è§£å†³å­—æ®µé¡ºåºé—®é¢˜ï¼‰
# ----------------------------
# åŠ è½½ç¼–ç å™¨å’Œè®­ç»ƒæ—¶çš„åˆ—å
if not os.path.exists(ONEHOT_ENCODER_PATH) or not os.path.exists(ONEHOT_FEATURES_PATH):
    raise FileNotFoundError(f"âŒ æœªæ‰¾åˆ°ç¼–ç å™¨æ–‡ä»¶ï¼Œè¯·ç¡®ä¿ {ONEHOT_ENCODER_PATH} å’Œ {ONEHOT_FEATURES_PATH} å­˜åœ¨")

encoder = joblib.load(ONEHOT_ENCODER_PATH)
encoder_columns = np.load(ONEHOT_FEATURES_PATH).tolist()

# å…³é”®ï¼šä¸¥æ ¼æŒ‰è®­ç»ƒæ—¶çš„å­—æ®µé¡ºåºæå–æ•°æ®ï¼Œç¡®ä¿ä¸encoder.fitæ—¶çš„é¡ºåºä¸€è‡´
# å³ä½¿merged_dfä¸­å­—æ®µé¡ºåºä¸åŒï¼Œä¹Ÿå¼ºåˆ¶æŒ‰ONEHOT_FIELDSé¡ºåºé€‰å–
onehot_input = merged_df[ONEHOT_FIELDS].fillna("").astype(str)

# æ‰§è¡Œç¼–ç ï¼ˆæ­¤æ—¶è¾“å…¥å­—æ®µé¡ºåºä¸è®­ç»ƒä¸€è‡´ï¼Œé¿å…Feature namesé¡ºåºé”™è¯¯ï¼‰
try:
    onehot_encoded = encoder.transform(onehot_input)
except ValueError as e:
    raise ValueError(f"âŒ ç¼–ç å¤±è´¥ï¼šè¾“å…¥å­—æ®µé¡ºåºä¸è®­ç»ƒæ—¶ä¸ä¸€è‡´ï¼Œè¯·æ£€æŸ¥ONEHOT_FIELDSæ˜¯å¦æ­£ç¡®ã€‚é”™è¯¯è¯¦æƒ…ï¼š{e}")

# ç”Ÿæˆä¸´æ—¶DataFrameï¼ˆåˆ—åä¸ºç¼–ç å™¨è‡ªåŠ¨ç”Ÿæˆï¼Œé¡ºåºä¸è®­ç»ƒä¸€è‡´ï¼‰
onehot_df = pd.DataFrame(onehot_encoded, columns=encoder.get_feature_names_out(ONEHOT_FIELDS))

# è¡¥é½è®­ç»ƒæ—¶çš„åˆ—ï¼ˆè‹¥æœ‰ç¼ºå¤±ï¼‰å¹¶å¼ºåˆ¶æŒ‰è®­ç»ƒé¡ºåºæ’åˆ—
for col in encoder_columns:
    if col not in onehot_df.columns:
        onehot_df[col] = 0
onehot_df = onehot_df[encoder_columns]  # æœ€ç»ˆé¡ºåºä¸è®­ç»ƒå®Œå…¨ä¸€è‡´

# æ‹¼æ¥è‡³åˆå¹¶æ•°æ®
merged_df = pd.concat([merged_df, onehot_df], axis=1)


# ----------------------------
# 5. ç‰¹å¾èåˆï¼ˆä½¿ç”¨è®­ç»ƒæ—¶çš„åˆ—åé¡ºåºï¼‰
# ----------------------------
def merge_features(row):
    code_emb = row["code_embedding"]
    text_embs = []
    for col in ["Desc_embedding", "Func_embedding", "case_spce_embedding", "case_purpose_embedding"]:
        text_embs.extend(row[col])
    onehot_embs = row[encoder_columns].tolist()  # æŒ‰è®­ç»ƒåˆ—åé¡ºåºæå–
    return code_emb + text_embs + onehot_embs

merged_df["merged_features"] = merged_df.apply(merge_features, axis=1)


# ----------------------------
# 6. ä¿å­˜ç»“æœ
# ----------------------------
processed_data_path = "data_to_infer.pkl"
merged_df.to_pickle(processed_data_path)
print(f"âœ… å¾…åˆ†ææ•°æ®å·²å¤„ç†å®Œæˆï¼Œä¿å­˜è‡³ {processed_data_path}")


#todoæ¨ç†çš„ç»“æœåŠ ä¸€ä¸ªä¼˜å…ˆçº§å’Œæ’åºç»“æœï¼Œä¼˜å…ˆçº§æ ¹æ®code_stræ˜¯å¦ä¸ºç©ºï¼Œå‡ºç°çš„æ²¡æœ‰è§è¿‡çš„onehotç¼–ç çš„æ•°é‡ç»™å®šä¸€ä¸ªå€¼ï¼ˆä¾‹å¦‚1ï¼Œ2ï¼Œ3ï¼Œ4ï¼‰
#todoå…ˆæ ¹æ®ä¼˜å…ˆçº§æ–°å‹æ’åºï¼ˆå€¼è¶Šå¤§ï¼Œä¼˜å…ˆçº§è¶Šé«˜ï¼‰ï¼Œå†…éƒ¨å†é€šè¿‡prob_0-prob_1è¿›è¡Œæ’åº


# ========================
# ğŸ”§ æ¨ç†é…ç½®ä¸å‡½æ•°ï¼ˆæ— ä¿®æ”¹ï¼‰
# ========================
MODEL_PATH = "best_log_classifier.pt"
DATA_PATH = "data_to_infer.pkl"
DEVICE = torch.device("npu:5" if torch.npu.is_available() else "cpu")
HIDDEN_DIM = 128
OUTPUT_PATH = "inference_results.csv"

def load_new_data(data_path):
    if not os.path.exists(data_path):
        raise FileNotFoundError(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
    data = pd.read_pickle(data_path)
    X_new = torch.tensor(data["merged_features"].tolist(), dtype=torch.float32)
    print(f"âœ… å·²åŠ è½½æ–°æ•°æ®ï¼Œå…± {len(X_new)} æ¡æ ·æœ¬ã€‚")
    return data, X_new

def predict_with_prob(model_path, data_tensor, hidden_dim=128):
    input_dim = data_tensor.shape[1]
    model = LogClassifier(input_dim=input_dim, hidden_dim=hidden_dim)
    model.load_state_dict(torch.load(model_path, map_location=DEVICE))
    model.to(DEVICE)
    model.eval()
    with torch.no_grad():
        outputs = model(data_tensor.to(DEVICE))
        probs = F.softmax(outputs, dim=1).cpu().numpy()
        preds = np.argmax(probs, axis=1)
    return preds, probs[:, 0], probs[:, 1]


# ----------------------------
# âœ… ä¿®æ”¹ï¼šå¢å¼ºä¼˜å…ˆçº§è®¡ç®—å‡½æ•°ï¼Œè¿”å›unseenæ˜ç»†
# ----------------------------
def calculate_priority_and_unseen(row, encoder, onehot_fields):
    """
    è®¡ç®—ä¼˜å…ˆçº§ï¼ˆ1-4çº§ï¼Œå€¼è¶Šå¤§ä¼˜å…ˆçº§è¶Šé«˜ï¼Œæ¨¡å‹ç»“æœè¶Šå¯é ï¼‰
    åŒæ—¶è¿”å›æœªè§è¿‡çš„å­—æ®µæ˜ç»†
    è¿”å›å€¼ï¼š(priority, unseen_count, unseen_fields, unseen_values)
    """
    # åŸºç¡€ä¼˜å…ˆçº§ï¼š4ï¼ˆæœ€å¯é ï¼‰
    priority = 4
    unseen_count = 0
    unseen_fields = []  # å­˜å‚¨æœªè§è¿‡çš„å­—æ®µå
    unseen_values = []  # å­˜å‚¨æœªè§è¿‡çš„å­—æ®µå¯¹åº”å€¼ï¼ˆä¸unseen_fieldsä¸€ä¸€å¯¹åº”ï¼‰

    # æ¡ä»¶1ï¼šcode_strä¸ºç©º â†’ å‡1
    code_str_empty = pd.isna(row["code_str"]) or row["code_str"].strip() == ""
    if code_str_empty:
        priority -= 1

    # æ¡ä»¶2ï¼šæ£€æŸ¥æ¯ä¸ªonehotå­—æ®µæ˜¯å¦æœ‰æœªè§è¿‡çš„å€¼
    for field in onehot_fields:
        field_value = str(row[field]).strip()
        field_idx = onehot_fields.index(field)
        known_categories = encoder.categories_[field_idx]
        # ç©ºå­—ç¬¦ä¸²æˆ–ä¸åœ¨å·²çŸ¥ç±»åˆ«ä¸­ â†’ è§†ä¸ºæœªè§è¿‡
        if field_value == "" or field_value not in known_categories:
            unseen_count += 1
            unseen_fields.append(field)
            unseen_values.append(field_value)

    # å‡å»æœªè§è¿‡çš„å­—æ®µæ•°é‡
    priority -= unseen_count

    # ç¡®ä¿ä¼˜å…ˆçº§ä¸ä½äº1
    priority = max(1, priority)

    # è¿”å›ä¼˜å…ˆçº§å’Œunseenæ˜ç»†
    return priority, unseen_count, unseen_fields, unseen_values

if __name__ == "__main__":
    print("ğŸš€ å¼€å§‹æ¨¡å‹æ¨ç†...")
    original_df, X_new = load_new_data(DATA_PATH)
    preds, prob_0, prob_1 = predict_with_prob(MODEL_PATH, X_new, hidden_dim=HIDDEN_DIM)
    print("âœ… æ¨ç†å®Œæˆï¼")


    result_df = original_df.copy()


    result_df["pred_label"] = preds
    result_df["prob_0"] = prob_0
    result_df["prob_1"] = prob_1

    # ----------------------------
    # âœ… è®¡ç®—ä¼˜å…ˆçº§ï¼ˆä¿®æ”¹åé€»è¾‘ï¼‰
    # ----------------------------
    print("ğŸ“Š è®¡ç®—æ ·æœ¬ä¼˜å…ˆçº§...")
    priority_list = []
    unseen_count_list = []
    unseen_fields_list = []
    unseen_values_list = []

    for idx, row in tqdm(result_df.iterrows(), total=len(result_df), desc="å¤„ç†æ ·æœ¬"):
        priority, unseen_count, unseen_fields, unseen_values = calculate_priority_and_unseen(
            row, encoder, ONEHOT_FIELDS
        )
        priority_list.append(priority)
        unseen_count_list.append(unseen_count)
        unseen_fields_list.append(unseen_fields)
        unseen_values_list.append(unseen_values)

    # å°†æ˜ç»†æ·»åŠ åˆ°ç»“æœè¡¨ä¸­
    result_df["priority"] = priority_list
    result_df["unseen_count"] = unseen_count_list
    result_df["unseen_fields"] = [",".join(fields) for fields in unseen_fields_list]  # è½¬ä¸ºå­—ç¬¦ä¸²ï¼Œæ–¹ä¾¿ä¿å­˜CSV
    result_df["unseen_values"] = [",".join(values) for values in unseen_values_list]  # è½¬ä¸ºå­—ç¬¦ä¸²ï¼Œæ–¹ä¾¿ä¿å­˜CSV

    # ----------------------------
    # âœ… è®¡ç®—æ’åºå…³é”®å­—ï¼ˆä¿æŒä¸å˜ï¼‰
    # ----------------------------
    result_df["sort_key"] = result_df["prob_0"] - result_df["prob_1"]

    # ----------------------------
    # âœ… æŒ‰ä¼˜å…ˆçº§å’Œæ’åºå…³é”®å­—æ’åºï¼ˆä¿æŒä¸å˜ï¼‰
    # ----------------------------
    # æ’åºè§„åˆ™ï¼š1. ä¼˜å…ˆçº§é™åºï¼ˆå€¼è¶Šå¤§è¶Šå¯é ï¼Œè¶Šé å‰ï¼‰ï¼›2. sort_keyé™åºï¼ˆåŒä¼˜å…ˆçº§å†…æ’åºï¼‰
    result_df_sorted = result_df.sort_values(
        by=["priority", "sort_key"],
        ascending=[False, False],
        ignore_index=True
    )


    # æœ€ç»ˆåªä¿ç•™ ["id", "data", "ts", "false_positive", "component", "rule","pred_label","prob_0","prob_1"]
    keep_cols = ["id", "data", "ts", "false_positive", "component", "rule",
                 "pred_label", "prob_0", "prob_1", "priority", "unseen_count",
                 "unseen_fields", "unseen_values", "sort_key"]
    result_df_sorted = result_df_sorted[keep_cols]
    result_df_sorted["sorted_rank"] = range(1, len(result_df_sorted) + 1)
    #è®¡ç®—ä¸€ä¸‹pred_labelå’Œfalse_positiveçš„ä¸€è‡´æƒ…å†µ
    # å¦‚æœfalse_positiveä¸ºfï¼Œé‚£ä¹ˆpred_labelä¸º0å°±æ˜¯ä¸€è‡´çš„
    # å¦‚æœfalse_positiveä¸ºtï¼Œé‚£ä¹ˆpred_labelä¸º1å°±æ˜¯ä¸€è‡´çš„
    # --- è®¡ç®— pred_label å’Œ false_positive æ˜¯å¦ä¸€è‡´ ---
    # #è®¡ç®—ä¸€ä¸‹å¬å›ç‡ï¼Œå‡†ç¡®åº¦ï¼Œç²¾ç¡®åº¦
    # result_df["is_match"] = result_df.apply(
    #     lambda row: (
    #             (row["false_positive"] == "f" and row["pred_label"] == 0) or
    #             (row["false_positive"] == "t" and row["pred_label"] == 1)
    #     ),
    #     axis=1
    # )
    #
    # # ğŸ“Œ è¿™é‡ŒåŠ å…¥ç»Ÿè®¡ä»£ç 
    # total = len(result_df)
    # match_count = result_df["is_match"].sum()
    # match_ratio = match_count / total if total > 0 else 0
    #
    # print(f"ğŸ”¢ ä¸€è‡´æ•°é‡ï¼ˆis_match=Trueï¼‰ï¼š{match_count}")
    # print(f"ğŸ“Š ä¸€è‡´å æ¯”ï¼š{match_ratio:.4f}  ï¼ˆçº¦ {match_ratio * 100:.2f}% ï¼‰")
    #
    # # --- è®¡ç®—è¯„ä¼°æŒ‡æ ‡ï¼ˆå¬å›ç‡ Recallã€å‡†ç¡®åº¦ Accuracyã€ç²¾ç¡®åº¦ Precisionï¼‰ ---
    #
    # # 1. å°† false_positive è½¬æ¢ä¸ºçœŸå®æ ‡ç­¾ y_true
    # result_df["true_label"] = result_df["false_positive"].map({"f": 0, "t": 1})
    #
    # y_true = result_df["true_label"].tolist()
    # y_pred = result_df["pred_label"].tolist()
    #
    # # 2. è®¡ç®— TP, FP, TN, FN
    # TP = sum((result_df["true_label"] == 1) & (result_df["pred_label"] == 1))
    # TN = sum((result_df["true_label"] == 0) & (result_df["pred_label"] == 0))
    # FP = sum((result_df["true_label"] == 0) & (result_df["pred_label"] == 1))
    # FN = sum((result_df["true_label"] == 1) & (result_df["pred_label"] == 0))
    #
    # # 3. æŒ‡æ ‡è®¡ç®—ï¼ˆé¿å…é™¤é›¶ï¼‰
    # accuracy = (TP + TN) / total if total > 0 else 0
    # recall = TP / (TP + FN) if (TP + FN) > 0 else 0
    # precision = TP / (TP + FP) if (TP + FP) > 0 else 0
    #
    # print("\nğŸ“Š â€”â€” è¯„ä¼°æŒ‡æ ‡ â€”â€”")
    # print(f"ğŸ¯ å‡†ç¡®åº¦ Accuracyï¼š{accuracy:.4f} ï¼ˆ{accuracy * 100:.2f}%ï¼‰")
    # print(f"ğŸ“ˆ å¬å›ç‡ Recallï¼š{recall:.4f} ï¼ˆ{recall * 100:.2f}%ï¼‰")
    # print(f"ğŸ¯ ç²¾ç¡®åº¦ Precisionï¼š{precision:.4f} ï¼ˆ{precision * 100:.2f}%ï¼‰")
    #
    # print(f"\nğŸ” TP={TP}, FP={FP}, TN={TN}, FN={FN}")

    # ----------------------------------------------------------
    result_df_sorted.to_csv(OUTPUT_PATH, index=False, encoding="utf-8-sig")
    print(f"ğŸ“„ é¢„æµ‹ç»“æœå·²ä¿å­˜è‡³ï¼š{OUTPUT_PATH}")
    print(f"æ ·ä¾‹é¢„è§ˆï¼š")
    print(result_df_sorted.head())