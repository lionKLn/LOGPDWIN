import torch
import pandas as pd
import numpy as np
import torch.nn.functional as F
from model import LogClassifier
import os
import joblib
from sklearn.preprocessing import OneHotEncoder


import os
import json
import pandas as pd
import torch
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModel
from torch_geometric.data import Batch
from unsupervised_train.preprocess import generate_graph_in_memory
from model import GAE_GIN

# ----------------------------
# é…ç½®å‚æ•°ï¼ˆå¼ºè°ƒå­—æ®µé¡ºåºå¿…é¡»ä¸è®­ç»ƒä¸€è‡´ï¼‰
# ----------------------------
INPUT_EXCEL = "path/to/your/input.xlsx"
UNSUPERVISED_MODEL_PATH = "logs/pdg/2025-05-20_14-30-00/best_pdg.pt"
DEVICE = torch.device("npu:4" if torch.npu.is_available() else "cpu")
EMBEDDING_DIM = 256
ONEHOT_ENCODER_PATH = "onehot_encoder.pkl"
ONEHOT_FEATURES_PATH = "onehot_feature_names.npy"
# å…³é”®ï¼šç¦»æ•£å­—æ®µé¡ºåºå¿…é¡»ä¸è®­ç»ƒæ—¶fitçš„é¡ºåºå®Œå…¨ä¸€è‡´ï¼ï¼ï¼
ONEHOT_FIELDS = ["component", "case_id", "test_suite", "rule"]  # é¡ºåºä¸å¯å˜


# ----------------------------
# 1. åŠ è½½Excelä¸è§£æJSON
# ----------------------------
df = pd.read_excel(INPUT_EXCEL)

results = []
for i, row in df.iterrows():
    try:
        data = json.loads(row["data"])
        raw_code = str(data.get("code_str", "")).strip()
        if raw_code.startswith('('):
            processed_code = raw_code
        elif raw_code.startswith('{'):
            processed_code = f"(){raw_code}"
        else:
            processed_code = f"(){{{raw_code}}}"

        results.append({
            "component": data.get("component", ""),
            "case_id": data.get("case_id", ""),
            "test_suite": data.get("test_suite", ""),
            "rule": data.get("rule", ""),  # æŒ‰ONEHOT_FIELDSé¡ºåºæ’åˆ—å­—æ®µ
            "code_str": processed_code,
            "raw_code": raw_code,
            "Desc": data.get("desc", ""),
            "Func": data.get("func", ""),
            "case_spce": data.get("case_spce", ""),
            "case_purpose": data.get("case_purpose", "")
        })
    except Exception as e:
        print(f"ç¬¬ {i} è¡Œ JSON è§£æå¤±è´¥: {e}")
        results.append({
            "component": "", "case_id": "", "test_suite": "", "rule": "",  # æŒ‰é¡ºåºå¡«å……é»˜è®¤å€¼
            "code_str": "", "raw_code": "",
            "Desc": "", "Func": "", "case_spce": "", "case_purpose": ""
        })

merged_df = pd.concat([df, pd.DataFrame(results)], axis=1)


# ----------------------------
# 2. ç”Ÿæˆä»£ç å›¾å¹¶ç¼–ç ï¼ˆæ— ä¿®æ”¹ï¼‰
# ----------------------------
print("å†…å­˜ä¸­ç”Ÿæˆ code_str çš„ä»£ç å›¾...")
graph_list = []
for idx, row in tqdm(merged_df.iterrows(), total=len(merged_df), desc="ç”Ÿæˆä»£ç å›¾"):
    processed_code = row["code_str"]
    if not processed_code:
        graph_list.append(None)
        continue

    torch_graph = generate_graph_in_memory(
        code_str=processed_code,
        func_name=f"func_{idx}"
    )
    graph_list.append(torch_graph)

print("åŠ è½½æ— ç›‘ç£å›¾ç¼–ç æ¨¡å‹...")
graph_model = GAE_GIN(
    in_channels=768,
    out_channels=768,
    device=DEVICE
).to(DEVICE)
graph_model.load_state_dict(torch.load(UNSUPERVISED_MODEL_PATH, map_location=DEVICE))
graph_model.eval()

print("ç¼–ç ä»£ç å›¾...")
code_embeddings = []
batch_size = 32

with torch.no_grad():
    for batch_start in tqdm(range(0, len(graph_list), batch_size), desc="ç¼–ç ä»£ç å›¾"):
        batch_graphs = graph_list[batch_start:batch_start + batch_size]
        valid_graphs, valid_indices = [], []
        for idx_in_batch, g in enumerate(batch_graphs):
            if g is not None:
                valid_graphs.append(g)
                valid_indices.append(idx_in_batch)

        batch_emb = [torch.zeros(EMBEDDING_DIM, device=DEVICE) for _ in batch_graphs]
        if valid_graphs:
            batch = Batch.from_data_list(valid_graphs).to(DEVICE)
            valid_embs = graph_model.forward(batch, mode="predict")
            for idx_in_batch, emb in zip(valid_indices, valid_embs):
                batch_emb[idx_in_batch] = emb

        batch_emb_cpu = [emb.cpu().tolist() for emb in batch_emb]
        code_embeddings.extend(batch_emb_cpu)

merged_df["code_embedding"] = code_embeddings


# ----------------------------
# 3. ç¼–ç æ–‡æœ¬å­—æ®µï¼ˆæ— ä¿®æ”¹ï¼‰
# ----------------------------
MODEL_PATH = "./models/paraphrase-multilingual-MiniLM-L12-v2"
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
text_model = AutoModel.from_pretrained(MODEL_PATH).to(DEVICE)
text_model.eval()

def mean_pooling(model_output, attention_mask):
    token_embeddings = model_output.last_hidden_state
    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, dim=1)
    sum_mask = torch.clamp(input_mask_expanded.sum(dim=1), min=1e-9)
    return sum_embeddings / sum_mask

def encode_texts(texts, tokenizer, model, device, batch_size=32, max_length=128, show_progress=True):
    all_embeddings = []
    with torch.no_grad():
        iterator = tqdm(range(0, len(texts), batch_size), desc="ç¼–ç æ–‡æœ¬") if show_progress else range(0, len(texts), batch_size)
        for start in iterator:
            batch_texts = texts[start:start + batch_size]
            encoded = tokenizer(batch_texts, padding=True, truncation=True, max_length=max_length, return_tensors="pt")
            input_ids, attention_mask = encoded["input_ids"].to(device), encoded["attention_mask"].to(device)
            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            sentence_embeddings = mean_pooling(outputs, attention_mask).cpu().tolist()
            all_embeddings.extend(sentence_embeddings)
    return all_embeddings

for col in ["Desc", "Func", "case_spce", "case_purpose"]:
    texts = merged_df[col].fillna("").astype(str).tolist()
    merged_df[col + "_embedding"] = encode_texts(texts, tokenizer, text_model, DEVICE)


# ----------------------------
# 4. One-hot ç¼–ç ï¼ˆå½»åº•è§£å†³å­—æ®µé¡ºåºé—®é¢˜ï¼‰
# ----------------------------
# åŠ è½½ç¼–ç å™¨å’Œè®­ç»ƒæ—¶çš„åˆ—å
if not os.path.exists(ONEHOT_ENCODER_PATH) or not os.path.exists(ONEHOT_FEATURES_PATH):
    raise FileNotFoundError(f"âŒ æœªæ‰¾åˆ°ç¼–ç å™¨æ–‡ä»¶ï¼Œè¯·ç¡®ä¿ {ONEHOT_ENCODER_PATH} å’Œ {ONEHOT_FEATURES_PATH} å­˜åœ¨")

encoder = joblib.load(ONEHOT_ENCODER_PATH)
encoder_columns = np.load(ONEHOT_FEATURES_PATH).tolist()

# å…³é”®ï¼šä¸¥æ ¼æŒ‰è®­ç»ƒæ—¶çš„å­—æ®µé¡ºåºæå–æ•°æ®ï¼Œç¡®ä¿ä¸encoder.fitæ—¶çš„é¡ºåºä¸€è‡´
# å³ä½¿merged_dfä¸­å­—æ®µé¡ºåºä¸åŒï¼Œä¹Ÿå¼ºåˆ¶æŒ‰ONEHOT_FIELDSé¡ºåºé€‰å–
onehot_input = merged_df[ONEHOT_FIELDS].fillna("").astype(str)

# æ‰§è¡Œç¼–ç ï¼ˆæ­¤æ—¶è¾“å…¥å­—æ®µé¡ºåºä¸è®­ç»ƒä¸€è‡´ï¼Œé¿å…Feature namesé¡ºåºé”™è¯¯ï¼‰
try:
    onehot_encoded = encoder.transform(onehot_input)
except ValueError as e:
    raise ValueError(f"âŒ ç¼–ç å¤±è´¥ï¼šè¾“å…¥å­—æ®µé¡ºåºä¸è®­ç»ƒæ—¶ä¸ä¸€è‡´ï¼Œè¯·æ£€æŸ¥ONEHOT_FIELDSæ˜¯å¦æ­£ç¡®ã€‚é”™è¯¯è¯¦æƒ…ï¼š{e}")

# ç”Ÿæˆä¸´æ—¶DataFrameï¼ˆåˆ—åä¸ºç¼–ç å™¨è‡ªåŠ¨ç”Ÿæˆï¼Œé¡ºåºä¸è®­ç»ƒä¸€è‡´ï¼‰
onehot_df = pd.DataFrame(onehot_encoded, columns=encoder.get_feature_names_out(ONEHOT_FIELDS))

# è¡¥é½è®­ç»ƒæ—¶çš„åˆ—ï¼ˆè‹¥æœ‰ç¼ºå¤±ï¼‰å¹¶å¼ºåˆ¶æŒ‰è®­ç»ƒé¡ºåºæ’åˆ—
for col in encoder_columns:
    if col not in onehot_df.columns:
        onehot_df[col] = 0
onehot_df = onehot_df[encoder_columns]  # æœ€ç»ˆé¡ºåºä¸è®­ç»ƒå®Œå…¨ä¸€è‡´

# æ‹¼æ¥è‡³åˆå¹¶æ•°æ®
merged_df = pd.concat([merged_df, onehot_df], axis=1)


# ----------------------------
# 5. ç‰¹å¾èåˆï¼ˆä½¿ç”¨è®­ç»ƒæ—¶çš„åˆ—åé¡ºåºï¼‰
# ----------------------------
def merge_features(row):
    code_emb = row["code_embedding"]
    text_embs = []
    for col in ["Desc_embedding", "Func_embedding", "case_spce_embedding", "case_purpose_embedding"]:
        text_embs.extend(row[col])
    onehot_embs = row[encoder_columns].tolist()  # æŒ‰è®­ç»ƒåˆ—åé¡ºåºæå–
    return code_emb + text_embs + onehot_embs

merged_df["merged_features"] = merged_df.apply(merge_features, axis=1)


# ----------------------------
# 6. ä¿å­˜ç»“æœ
# ----------------------------
processed_data_path = "data_to_infer.pkl"
merged_df.to_pickle(processed_data_path)
print(f"âœ… å¾…åˆ†ææ•°æ®å·²å¤„ç†å®Œæˆï¼Œä¿å­˜è‡³ {processed_data_path}")


# ========================
# ğŸ”§ æ¨ç†é…ç½®ä¸å‡½æ•°ï¼ˆæ— ä¿®æ”¹ï¼‰
# ========================
MODEL_PATH = "best_log_classifier.pt"
DATA_PATH = "data_to_infer.pkl"
DEVICE = torch.device("npu:5" if torch.npu.is_available() else "cpu")
HIDDEN_DIM = 128
OUTPUT_PATH = "inference_results.csv"

def load_new_data(data_path):
    if not os.path.exists(data_path):
        raise FileNotFoundError(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
    data = pd.read_pickle(data_path)
    X_new = torch.tensor(data["merged_features"].tolist(), dtype=torch.float32)
    print(f"âœ… å·²åŠ è½½æ–°æ•°æ®ï¼Œå…± {len(X_new)} æ¡æ ·æœ¬ã€‚")
    return data, X_new

def predict_with_prob(model_path, data_tensor, hidden_dim=128):
    input_dim = data_tensor.shape[1]
    model = LogClassifier(input_dim=input_dim, hidden_dim=hidden_dim)
    model.load_state_dict(torch.load(model_path, map_location=DEVICE))
    model.to(DEVICE)
    model.eval()
    with torch.no_grad():
        outputs = model(data_tensor.to(DEVICE))
        probs = F.softmax(outputs, dim=1).cpu().numpy()
        preds = np.argmax(probs, axis=1)
    return preds, probs[:, 0], probs[:, 1]

if __name__ == "__main__":
    print("ğŸš€ å¼€å§‹æ¨¡å‹æ¨ç†...")
    original_df, X_new = load_new_data(DATA_PATH)
    preds, prob_0, prob_1 = predict_with_prob(MODEL_PATH, X_new, hidden_dim=HIDDEN_DIM)
    print("âœ… æ¨ç†å®Œæˆï¼")
    result_df = original_df.copy()
    result_df["pred_label"] = preds
    result_df["prob_0"] = prob_0
    result_df["prob_1"] = prob_1
    result_df.to_csv(OUTPUT_PATH, index=False, encoding="utf-8-sig")
    print(f"ğŸ“„ é¢„æµ‹ç»“æœå·²ä¿å­˜è‡³ï¼š{OUTPUT_PATH}")
    print(f"æ ·ä¾‹é¢„è§ˆï¼š")
    print(result_df.head())