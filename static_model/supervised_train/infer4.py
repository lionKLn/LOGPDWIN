import torch
import pandas as pd
import numpy as np
import torch.nn.functional as F
from model import LogClassifier
import os
import joblib
from sklearn.preprocessing import OneHotEncoder

import json
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModel
from torch_geometric.data import Batch
from unsupervised_train.preprocess import generate_graph_in_memory
from model import GAE_GIN

# ----------------------------
# é…ç½®å‚æ•°ï¼ˆå¼ºè°ƒå­—æ®µé¡ºåºå¿…é¡»ä¸è®­ç»ƒä¸€è‡´ï¼‰
# ----------------------------
INPUT_EXCEL = "path/to/your/input.xlsx"
UNSUPERVISED_MODEL_PATH = "logs/pdg/2025-05-20_14-30-00/best_pdg.pt"
DEVICE = torch.device("npu:4" if torch.npu.is_available() else "cpu")
EMBEDDING_DIM = 256
ONEHOT_ENCODER_PATH = "onehot_encoder.pkl"
ONEHOT_FEATURES_PATH = "onehot_feature_names.npy"
# å…³é”®ï¼šç¦»æ•£å­—æ®µé¡ºåºå¿…é¡»ä¸è®­ç»ƒæ—¶fitçš„é¡ºåºå®Œå…¨ä¸€è‡´ï¼ï¼ï¼
ONEHOT_FIELDS = ["component", "case_id", "test_suite", "rule"]  # é¡ºåºä¸å¯å˜

# ----------------------------
# 1. åŠ è½½Excelä¸è§£æJSON
# ----------------------------
# è¯»å–æœ€åŸå§‹çš„ excel æ•°æ®ï¼ˆä¿ç•™ä½œä¸ºæœ€ç»ˆè¾“å‡ºçš„åŸºç¡€ï¼‰
orig_df = pd.read_excel(INPUT_EXCEL)

# æŒ‰åŸé€»è¾‘è§£æ data åˆ—ä¸­çš„ JSONï¼Œç”Ÿæˆæ‰©å±•å­—æ®µï¼ˆmerged_df æ˜¯åç»­å¤„ç†æ•°æ®ï¼‰
results = []
for i, row in orig_df.iterrows():
    try:
        data = json.loads(row["data"])
        raw_code = str(data.get("code_str", "")).strip()
        if raw_code.startswith('('):
            processed_code = raw_code
        elif raw_code.startswith('{'):
            processed_code = f"(){raw_code}"
        else:
            processed_code = f"(){{{raw_code}}}"

        results.append({
            "component": data.get("component", ""),
            "case_id": data.get("case_id", ""),
            "test_suite": data.get("test_suite", ""),
            "rule": data.get("rule", ""),  # æŒ‰ONEHOT_FIELDSé¡ºåºæ’åˆ—å­—æ®µ
            "code_str": processed_code,
            "raw_code": raw_code,
            "Desc": data.get("desc", ""),
            "Func": data.get("func", ""),
            "case_spce": data.get("case_spce", ""),
            "case_purpose": data.get("case_purpose", "")
        })
    except Exception as e:
        print(f"ç¬¬ {i} è¡Œ JSON è§£æå¤±è´¥: {e}")
        results.append({
            "component": "", "case_id": "", "test_suite": "", "rule": "",
            "code_str": "", "raw_code": "",
            "Desc": "", "Func": "", "case_spce": "", "case_purpose": ""
        })

# merged_df = åŸå§‹ excel è¡Œ + è§£æå‡ºçš„æ‰©å±•å­—æ®µ
merged_df = pd.concat([orig_df.reset_index(drop=True), pd.DataFrame(results)], axis=1)

# ----------------------------
# 2. ç”Ÿæˆä»£ç å›¾å¹¶ç¼–ç ï¼ˆæ— ä¿®æ”¹ï¼‰
# ----------------------------
print("å†…å­˜ä¸­ç”Ÿæˆ code_str çš„ä»£ç å›¾...")
graph_list = []
for idx, row in tqdm(merged_df.iterrows(), total=len(merged_df), desc="ç”Ÿæˆä»£ç å›¾"):
    processed_code = row.get("code_str", "")
    if not processed_code:
        graph_list.append(None)
        continue

    torch_graph = generate_graph_in_memory(
        code_str=processed_code,
        func_name=f"func_{idx}"
    )
    graph_list.append(torch_graph)

print("åŠ è½½æ— ç›‘ç£å›¾ç¼–ç æ¨¡å‹...")
graph_model = GAE_GIN(
    in_channels=768,
    out_channels=768,
    device=DEVICE
).to(DEVICE)
graph_model.load_state_dict(torch.load(UNSUPERVISED_MODEL_PATH, map_location=DEVICE))
graph_model.eval()

print("ç¼–ç ä»£ç å›¾...")
code_embeddings = []
batch_size = 32

with torch.no_grad():
    for batch_start in tqdm(range(0, len(graph_list), batch_size), desc="ç¼–ç ä»£ç å›¾"):
        batch_graphs = graph_list[batch_start:batch_start + batch_size]
        valid_graphs, valid_indices = [], []
        for idx_in_batch, g in enumerate(batch_graphs):
            if g is not None:
                valid_graphs.append(g)
                valid_indices.append(idx_in_batch)

        batch_emb = [torch.zeros(EMBEDDING_DIM, device=DEVICE) for _ in batch_graphs]
        if valid_graphs:
            batch = Batch.from_data_list(valid_graphs).to(DEVICE)
            valid_embs = graph_model.forward(batch, mode="predict")
            for idx_in_batch, emb in zip(valid_indices, valid_embs):
                batch_emb[idx_in_batch] = emb

        batch_emb_cpu = [emb.cpu().tolist() for emb in batch_emb]
        code_embeddings.extend(batch_emb_cpu)

merged_df["code_embedding"] = code_embeddings

# ----------------------------
# 3. ç¼–ç æ–‡æœ¬å­—æ®µï¼ˆæ— ä¿®æ”¹ï¼‰
# ----------------------------
MODEL_PATH = "./models/paraphrase-multilingual-MiniLM-L12-v2"
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
text_model = AutoModel.from_pretrained(MODEL_PATH).to(DEVICE)
text_model.eval()

def mean_pooling(model_output, attention_mask):
    token_embeddings = model_output.last_hidden_state
    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, dim=1)
    sum_mask = torch.clamp(input_mask_expanded.sum(dim=1), min=1e-9)
    return sum_embeddings / sum_mask

def encode_texts(texts, tokenizer, model, device, batch_size=32, max_length=128, show_progress=True):
    all_embeddings = []
    with torch.no_grad():
        iterator = tqdm(range(0, len(texts), batch_size), desc="ç¼–ç æ–‡æœ¬") if show_progress else range(0, len(texts), batch_size)
        for start in iterator:
            batch_texts = texts[start:start + batch_size]
            encoded = tokenizer(batch_texts, padding=True, truncation=True, max_length=max_length, return_tensors="pt")
            input_ids, attention_mask = encoded["input_ids"].to(device), encoded["attention_mask"].to(device)
            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            sentence_embeddings = mean_pooling(outputs, attention_mask).cpu().tolist()
            all_embeddings.extend(sentence_embeddings)
    return all_embeddings

for col in ["Desc", "Func", "case_spce", "case_purpose"]:
    texts = merged_df[col].fillna("").astype(str).tolist()
    merged_df[col + "_embedding"] = encode_texts(texts, tokenizer, text_model, DEVICE)

# ----------------------------
# 4. One-hot ç¼–ç ï¼ˆå½»åº•è§£å†³å­—æ®µé¡ºåºé—®é¢˜ï¼‰
# ----------------------------
if not os.path.exists(ONEHOT_ENCODER_PATH) or not os.path.exists(ONEHOT_FEATURES_PATH):
    raise FileNotFoundError(f"âŒ æœªæ‰¾åˆ°ç¼–ç å™¨æ–‡ä»¶ï¼Œè¯·ç¡®ä¿ {ONEHOT_ENCODER_PATH} å’Œ {ONEHOT_FEATURES_PATH} å­˜åœ¨")

encoder = joblib.load(ONEHOT_ENCODER_PATH)
encoder_columns = np.load(ONEHOT_FEATURES_PATH).tolist()

# å¼ºåˆ¶æŒ‰è®­ç»ƒæ—¶çš„å­—æ®µé¡ºåºæå–æ•°æ®
onehot_input = merged_df[ONEHOT_FIELDS].fillna("").astype(str)

try:
    onehot_encoded = encoder.transform(onehot_input)
except ValueError as e:
    raise ValueError(f"âŒ ç¼–ç å¤±è´¥ï¼šè¾“å…¥å­—æ®µä¸è®­ç»ƒæ—¶ä¸ä¸€è‡´ï¼Œè¯·æ£€æŸ¥ ONEHOT_FIELDSã€‚é”™è¯¯è¯¦æƒ…ï¼š{e}")

onehot_df = pd.DataFrame(onehot_encoded, columns=encoder.get_feature_names_out(ONEHOT_FIELDS))

# è¡¥é½è®­ç»ƒæ—¶çš„åˆ—å¹¶æŒ‰è®­ç»ƒåˆ—æ’åº
for col in encoder_columns:
    if col not in onehot_df.columns:
        onehot_df[col] = 0
onehot_df = onehot_df[encoder_columns]

merged_df = pd.concat([merged_df, onehot_df], axis=1)

# ----------------------------
# 5. ç‰¹å¾èåˆï¼ˆä½¿ç”¨è®­ç»ƒæ—¶çš„åˆ—åé¡ºåºï¼‰
# ----------------------------
def merge_features(row):
    code_emb = row["code_embedding"]
    text_embs = []
    for col in ["Desc_embedding", "Func_embedding", "case_spce_embedding", "case_purpose_embedding"]:
        text_embs.extend(row[col])
    onehot_embs = row[encoder_columns].tolist()
    return code_emb + text_embs + onehot_embs

merged_df["merged_features"] = merged_df.apply(merge_features, axis=1)

# ----------------------------
# 6. ä¿å­˜é¢„å¤„ç†ç»“æœï¼ˆä¿å­˜ åŸå§‹ orig_df å’Œ merged_dfï¼‰
# ----------------------------
processed_data_path = "data_to_infer.pkl"
# ä¿å­˜ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å«æœ€åŸå§‹çš„ orig_dfï¼ˆç”¨äºæœ€ç»ˆè¾“å‡ºï¼‰å’Œ merged_dfï¼ˆç”¨äºæ¨ç†ï¼‰
to_save = {
    "orig_df": orig_df,
    "merged_df": merged_df
}
pd.to_pickle(to_save, processed_data_path)
print(f"âœ… å¾…åˆ†ææ•°æ®å·²å¤„ç†å®Œæˆï¼Œä¿å­˜è‡³ {processed_data_path}")

# ========================
# ğŸ”§ æ¨ç†é…ç½®ä¸å‡½æ•°
# ========================
MODEL_PATH = "best_log_classifier.pt"
DATA_PATH = processed_data_path
DEVICE = torch.device("npu:5" if torch.npu.is_available() else "cpu")
HIDDEN_DIM = 128
OUTPUT_PATH = "inference_results.csv"

def load_new_data(data_path):
    if not os.path.exists(data_path):
        raise FileNotFoundError(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
    loaded = pd.read_pickle(data_path)
    # å…¼å®¹ä¸¤ç§æƒ…å†µï¼š1) æˆ‘ä»¬ä¿å­˜çš„ dict 2) ç›´æ¥ä¿å­˜çš„ DataFrameï¼ˆå‘åå…¼å®¹ï¼‰
    if isinstance(loaded, dict):
        orig = loaded.get("orig_df")
        merged = loaded.get("merged_df")
    else:
        # è‹¥æ˜¯æ—§ç‰ˆç›´æ¥ä¿å­˜ merged_df çš„æƒ…å†µï¼Œåˆ™å°è¯•ä» merged ä¸­æ¢å¤åŸå§‹åˆ—ï¼ˆè‹¥å­˜åœ¨ï¼‰
        merged = loaded
        # å°è¯•è¯†åˆ«æœ€åˆçš„åŸå§‹åˆ—ï¼šè¿™é‡Œä¼˜å…ˆåˆ¤æ–­æ˜¯å¦ä¿å­˜è¿‡åŸå§‹åˆ—é›† metadataï¼ˆè‹¥æ²¡æœ‰ï¼Œä»è¿”å› mergedï¼‰
        orig = None
    if merged is None:
        raise ValueError("âŒ æœªèƒ½ä»ä¿å­˜æ–‡ä»¶ä¸­æ‰¾åˆ° merged_df")
    X_new = torch.tensor(merged["merged_features"].tolist(), dtype=torch.float32)
    print(f"âœ… å·²åŠ è½½æ–°æ•°æ®ï¼Œå…± {len(X_new)} æ¡æ ·æœ¬ã€‚")
    return orig, merged, X_new

def predict_with_prob(model_path, data_tensor, hidden_dim=128):
    input_dim = data_tensor.shape[1]
    model = LogClassifier(input_dim=input_dim, hidden_dim=hidden_dim)
    model.load_state_dict(torch.load(model_path, map_location=DEVICE))
    model.to(DEVICE)
    model.eval()
    with torch.no_grad():
        outputs = model(data_tensor.to(DEVICE))
        probs = F.softmax(outputs, dim=1).cpu().numpy()
        preds = np.argmax(probs, axis=1)
    return preds, probs[:, 0], probs[:, 1]

if __name__ == "__main__":
    print("ğŸš€ å¼€å§‹æ¨¡å‹æ¨ç†...")
    orig_loaded, merged_loaded, X_new = load_new_data(DATA_PATH)

    preds, prob_0, prob_1 = predict_with_prob(MODEL_PATH, X_new, hidden_dim=HIDDEN_DIM)
    print("âœ… æ¨ç†å®Œæˆï¼")

    # ç¡®ä¿é•¿åº¦åŒ¹é…
    n_samples = len(preds)
    if orig_loaded is None:
        # å¦‚æœæ²¡æœ‰åŸå§‹ orig_dfï¼ˆæç«¯æƒ…å†µï¼‰ï¼Œå°è¯•ä» merged_loaded æ¢å¤æœ€åˆè¯»å–çš„åˆ—
        # è¿™é‡Œå‡å®šåŸå§‹åˆ—åœ¨ merged_loaded çš„å‰ len(orig_df_columns) åˆ—ï¼ˆè‹¥æ— æ³•æ¢å¤ï¼Œä»ä¿å­˜ merged_loaded çš„ç´¢å¼•åˆ— + predsï¼‰
        print("âš ï¸ æœªåœ¨ä¿å­˜æ–‡ä»¶ä¸­æ‰¾åˆ°åŸå§‹ orig_dfï¼Œç»“æœä¼šä½¿ç”¨ merged_df ä¸­çš„å‰å‡ åˆ—ï¼ˆå¦‚æœå­˜åœ¨ï¼‰ä½œä¸ºåŸºç¡€ã€‚")
        base_df = merged_loaded.copy().reset_index(drop=True)
    else:
        base_df = orig_loaded.copy().reset_index(drop=True)

    # æ£€æŸ¥æ ·æœ¬æ•°é‡ä¸€è‡´æ€§
    if len(base_df) != n_samples:
        # å¦‚æœä¸ä¸€è‡´ï¼Œå°è¯•ç”¨ç´¢å¼•å¯¹é½ï¼šä»¥è¾ƒå°é•¿åº¦ä¸ºå‡†
        min_len = min(len(base_df), n_samples)
        print(f"âš ï¸ æ•°æ®æ¡ç›®æ•°é‡ä¸ä¸€è‡´ï¼šorig {len(base_df)} vs preds {n_samples}ï¼Œå°†æŒ‰æœ€å°å€¼ {min_len} æˆªæ–­ä¿å­˜ã€‚")
        base_df = base_df.iloc[:min_len].reset_index(drop=True)
        preds = preds[:min_len]
        prob_0 = prob_0[:min_len]
        prob_1 = prob_1[:min_len]

    # åªä¿ç•™æœ€åˆè¯»å–çš„åˆ— + é¢„æµ‹ç»“æœåˆ—
    result_df = base_df.copy()
    result_df["pred_label"] = preds
    result_df["prob_0"] = prob_0
    result_df["prob_1"] = prob_1

    # ä¿å­˜ CSVï¼ˆä»…åŒ…å«åŸå§‹åˆ—ä¸ä¸‰åˆ—é¢„æµ‹ç»“æœï¼‰
    result_df.to_csv(OUTPUT_PATH, index=False, encoding="utf-8-sig")
    print(f"ğŸ“„ é¢„æµ‹ç»“æœå·²ä¿å­˜è‡³ï¼š{OUTPUT_PATH}")
    print("æ ·ä¾‹é¢„è§ˆï¼š")
    print(result_df.head())
